{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Guard - The Security Toolkit for LLM Interactions","text":"<p>LLM-Guard is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).</p>"},{"location":"#what-is-llm-guard","title":"What is LLM Guard?","text":"<p>By offering sanitization, detection of harmful language, prevention of data leakage, and resistance against prompt injection attacks, LLM-Guard ensures that your interactions with LLMs remain safe and secure.</p> <p>Demo</p>"},{"location":"#installation","title":"Installation","text":"<p>Begin your journey with LLM Guard by downloading the package and acquiring the <code>en_core_web_trf</code> spaCy model (essential for the Anonymize scanner):</p> <pre><code>pip install llm-guard\npython -m spacy download en_core_web_trf\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Important Notes:</p> <ul> <li>LLM Guard is designed for easy integration and deployment in production environments. While it's ready to use   out-of-the-box, please be informed that we're constantly improving and updating the repository.</li> <li>Base functionality requires a limited number of libraries. As you explore more advanced features, necessary libraries   will be automatically installed.</li> <li>Ensure you're using Python version 3.8.1 or higher. Confirm with: <code>python --version</code>.</li> <li>Library installation issues? Consider upgrading pip: <code>python -m pip install --upgrade pip</code>.</li> </ul> <p>Examples:</p> <ul> <li>Get started with ChatGPT and LLM Guard.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<p>General:</p> <ul> <li> Introduce support of GPU</li> <li> Improve documentation by showing use-cases, benchmarks, etc</li> <li> Hosted version of LLM Guard</li> <li> Text statistics to provide on prompt and output</li> <li> Support more languages</li> <li> Accept multiple outputs instead of one to compare</li> <li> Support streaming mode</li> </ul> <p>Prompt Scanner:</p> <ul> <li> Integrate with Perspective API for Toxicity scanner</li> <li> Develop language restricting scanner</li> </ul> <p>Output Scanner:</p> <ul> <li> Develop output scanners for the format (e.g. max length, correct JSON, XML, etc)</li> <li> Develop factual consistency scanner</li> <li> Develop libraries hallucination scanner</li> <li> Develop libraries licenses scanner</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Got ideas, feedback, or wish to contribute? We'd love to hear from you! Email us.</p> <p>For detailed guidelines on contributions, kindly refer to our contribution guide.</p>"},{"location":"installation/","title":"Installing LLM Guard","text":"<p>This document describes how to download and install the LLM Guard locally.</p>"},{"location":"installation/#supported-python-versions","title":"Supported Python Versions","text":"<p>LLM Guard is supported for the following python versions:</p> <ul> <li>3.10</li> <li>3.11</li> </ul>"},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>Note</p> <p>Consider installing the LLM Guard python packages on a virtual environment like <code>venv</code> or <code>conda</code>.</p> <pre><code>pip install llm-guard\n\n# LLM Guard Anonymize scanner requires a spaCy language model:\npython -m spacy download en_core_web_trf\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>To install LLM Guard from source, first clone the repo:</p> <ul> <li>Using HTTPS <pre><code>git clone https://github.com/laiyer-ai/llm-guard.git\n</code></pre></li> <li>Using SSH <pre><code>git clone git@github.com:laiyer-ai/llm-guard.git\n</code></pre></li> </ul> <p>Then, install the package using <code>pip</code>:</p> <pre><code># install the repo\npip install -U -r requirements.txt -r requirements-dev.txt\npython setup.py install\n</code></pre> <p>Then, download the Spacy model for the <code>Anonymize</code> scanner:</p> <pre><code># download SpaCy model\npython -m spacy download en_core_web_trf\n</code></pre>"},{"location":"quickstart/","title":"Getting started with LLM Guard","text":"<p>Each scanner can be used individually, or using the <code>scan_prompt</code> function.</p>"},{"location":"quickstart/#individual","title":"Individual","text":"<p>You can import an individual scanner and use it to evaluate the prompt or the output:</p> <pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"quickstart/#multiple","title":"Multiple","text":"<p>!! info</p> <pre><code>Scanners are executed in the order they are passed to the `scan_prompt` function.\n</code></pre> <p>For prompt:</p> <pre><code>from llm_guard import scan_prompt\nfrom llm_guard.input_scanners import Anonymize, PromptInjection, TokenLimit, Toxicity\nfrom llm_guard.vault import Vault\n\nvault = Vault()\ninput_scanners = [Anonymize(vault), Toxicity(), TokenLimit(), PromptInjection()]\n\nsanitized_prompt, results_valid, results_score = scan_prompt(input_scanners, prompt)\nif any(not result for result in results_valid.values()):\n    print(f\"Prompt {prompt} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Prompt: {sanitized_prompt}\")\n</code></pre> <p>For output:</p> <pre><code>from llm_guard import scan_output\nfrom llm_guard.output_scanners import Deanonymize, NoRefusal, Relevance, Sensitive\n\nvault = Vault()\noutput_scanners = [Deanonymize(vault), NoRefusal(), Relevance(), Sensitive()]\n\nsanitized_response_text, results_valid, results_score = scan_output(\n    output_scanners, sanitized_prompt, response_text\n)\nif any(not result for result in results_valid.values()):\n    print(f\"Output {response_text} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Output: {sanitized_response_text}\\n\")\n</code></pre> <p>Note</p> <p>You can set <code>fail_fast</code> to <code>True</code> to stop scanning after the first invalid result. This can help to reduce the latency of the scanning.</p>"},{"location":"benchmarks/input_scanners/","title":"Input scanners benchmarks","text":"<p>Info</p> <p>Reference code: https://github.com/laiyer-ai/llm-guard/blob/main/benchmarks/scan_prompt_latency.py</p>"},{"location":"benchmarks/input_scanners/#test-environment","title":"Test environment","text":"<ul> <li>Platform: Linux</li> <li>Python Version: 3.10.2</li> </ul>"},{"location":"benchmarks/input_scanners/#setup","title":"Setup","text":"<ul> <li>Basic scanners: Anonymize, Secrets.</li> <li>Intermediate   scanners: Anonymize, Secrets, PromptInjection, Toxicity.</li> <li>Advanced scanners: All scanners.</li> </ul>"},{"location":"benchmarks/input_scanners/#instance-type-aws-m6glarge-graviton","title":"Instance type: AWS <code>m6g.large</code> (Graviton)","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic Make an SQL insert statement... 360.56 404.38 369.66 361.09 19.41 Basic For some reason, the assistant is... 517.72 521.56 519.09 518.67 1.52 Intermediate Make an SQL insert statement... 1,221.92 1,227 1,224.71 1,225.43 2.03 Intermediate For some reason, the assistant is... 1,549.82 2,325.64 1,929.78 1,919.46 306.18 Advanced Make an SQL insert statement... 2,167.89 2,180.68 2,172.99 2,172.05 5.07 Advanced For some reason, the assistant is... 2,662.04 2,879.55 2,764.91 2,757.08 86.88"},{"location":"benchmarks/input_scanners/#aws-m5large-instance","title":"AWS <code>m5.large</code> instance","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic Make an SQL insert statement... 211.75 252.38 220.28 212.71 17.94 Basic For some reason, the assistant is... 317.44 323.65 320.08 318.84 2.92 Intermediate Make an SQL insert statement... 620.28 632.61 627.26 629.0 4.8 Intermediate For some reason, the assistant is... 782.69 1,101.70 937.67 924.63 127.35 Advanced Make an SQL insert statement... 1,145.41 1,184.17 1,160.3 1,157.22 14.63 Advanced For some reason, the assistant is... 1,377.93 1,514.38 1,449.14 1,448.96 52.77"},{"location":"benchmarks/input_scanners/#aws-inf1xlarge-instance","title":"AWS <code>inf1.xlarge</code> instance","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic Make an SQL insert statement... 112.86 139.02 118.21 112.93 11.63 Basic For some reason, the assistant is... 164.84 171.22 166.99 166.71 2.6 Intermediate Make an SQL insert statement... 312.64 321.35 314.9 313.26 3.65 Intermediate For some reason, the assistant is... 399.14 540.05 471.80 474.01 55.17 Advanced Make an SQL insert statement... 564.13 612.13 574.33 564.80 21.15 Advanced For some reason, the assistant is... 665.89 729.69 694.36 689.37 25.19"},{"location":"benchmarks/output_scanners/","title":"Output scanners benchmarks","text":"<p>Info</p> <p>Reference code: https://github.com/laiyer-ai/llm-guard/blob/main/benchmarks/scan_output_latency.py</p>"},{"location":"benchmarks/output_scanners/#test-environment","title":"Test environment","text":"<ul> <li>Platform: Linux</li> <li>Python Version: 3.10.2</li> </ul>"},{"location":"benchmarks/output_scanners/#setup","title":"Setup","text":"<ul> <li>Basic scanners: Deanonymize, Secrets.</li> <li>Intermediate   scanners: Deanonymize, Secrets, Bias, Toxicity.</li> <li>Advanced scanners: All scanners.</li> </ul>"},{"location":"benchmarks/output_scanners/#instance-type-aws-m6glarge-graviton","title":"Instance type: AWS <code>m6g.large</code> (Graviton)","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic INSERT INTO users (Name, Email... 338.90 370.01 345.97 339.742 13.47 Intermediate INSERT INTO users (Name, Email... 1,221.92 1,227 1,224.71 1,225.43 2.03 Advanced INSERT INTO users (Name, Email... 4,037.6 4,170.9 4,074.3 4,056.4 5.49"},{"location":"benchmarks/output_scanners/#aws-m5large-instance","title":"AWS <code>m5.large</code> instance","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic INSERT INTO users (Name, Email... 206.15 256.63 216.35 206.25 22.51 Intermediate INSERT INTO users (Name, Email... 734.10 745.58 740.73 740.84 4.58 Advanced INSERT INTO users (Name, Email... 2,331.6 2,500.7 2,373.0 2,338.7 7.2"},{"location":"benchmarks/output_scanners/#aws-inf1xlarge-instance","title":"AWS <code>inf1.xlarge</code> instance","text":"Test Type Prompt (Trimmed) Min (ms) Max (ms) Mean (ms) Median StdDev (ms) Basic INSERT INTO users (Name, Email... 107.29 156.32 117.88 108.43 21.5 Intermediate INSERT INTO users (Name, Email... 388.51 397.73 392.32 391.63 3.44 Advanced INSERT INTO users (Name, Email... 816.51 913.67 837.71 819.98 42.49"},{"location":"customization/add_scanner/","title":"Adding a new scanner","text":"<p>LLM Guard can be extended to support new scanners, and to support additional models for the existing. These scanners could be added via code or ad-hoc as part of the request.</p> <p>Note</p> <p>Before writing code, please read the contributing guide.</p>"},{"location":"customization/add_scanner/#extending-the-input-prompt-scanners","title":"Extending the input (prompt) scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/input_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/input_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/input_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/input_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>CHANGELOG.md</code> file.</li> </ol>"},{"location":"customization/add_scanner/#extending-the-output-scanners","title":"Extending the output scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/output_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/output_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/output_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/output_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>CHANGELOG.md</code> file.</li> </ol> <p>Info</p> <p>You can use existing scanners as a reference.</p>"},{"location":"input_scanners/anonymize/","title":"Anonymize Scanner","text":"<p>It meticulously inspects user prompts, ensuring that they are free of any sensitive information before being processed by the language model.</p>"},{"location":"input_scanners/anonymize/#attack","title":"Attack","text":"<p>Sometimes, Language Learning Models (or LLMs) can accidentally share private info from the prompts they get. This can be bad because it might let others see or use this info in the wrong way.</p> <p>To stop this from happening, we use the Anonymize scanner. It makes sure user prompts don\u2019t have any private details before the model sees them.</p>"},{"location":"input_scanners/anonymize/#usage","title":"Usage","text":"<p>When you use the Anonymize scanner, you can talk to LLMs without worrying about accidentally sharing private info.</p> <p>The scanner uses a tool called the Presidio Analyzer library. This tool, built with Python's spaCy, is good at finding private info in text.</p> <p>Note</p> <p>It uses transformers based model <code>en_core_web_trf</code> which uses a more modern deep-learning architecture, but is generally slower than the default <code>en_core_web_lg</code> model.</p> <p>On top of that, the Anonymize scanner can also understand special patterns to catch anything the Presidio Analyzer might miss.</p>"},{"location":"input_scanners/anonymize/#entities","title":"Entities","text":"<p>Note</p> <p>Entity detection only works in English right now</p> <ul> <li>List of default entities</li> <li>Supported Presidio entities</li> <li>Custom regex patterns</li> </ul>"},{"location":"input_scanners/anonymize/#configuring","title":"Configuring","text":"<p>First, set up the Vault. It keeps a record of the info we redact:</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Then, set up the Anonymize scanner with some options:</p> <pre><code>from llm_guard.input_scanners import Anonymize\n\nscanner = Anonymize(vault, preamble=\"Insert before prompt\", allowed_names=[\"John Doe\"], hidden_names=[\"Test LLC\"])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Here's what those options do:</p> <ul> <li><code>preamble</code> tells the LLM to ignore certain things.</li> <li><code>hidden_names</code> are names we change to something like <code>[REDACTED_CUSTOM_1]</code>.</li> <li>You can also choose specific types of info to hide using <code>entity_types</code>.</li> <li>If you want, you can use your own patterns by giving the path in <code>regex_pattern_groups_path</code>.</li> <li><code>use_faker</code> will replace applicable entities with fake ones.</li> </ul> <p>If you want to see the original info again, you can use the Deanonymizer.</p>"},{"location":"input_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>Ensure that specific undesired substrings never make it into your prompts with the BanSubstrings scanner.</p>"},{"location":"input_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It is purpose-built to screen user prompts, ensuring none of the banned substrings are present. Users have the flexibility to enforce this check at two distinct granularity levels:</p> <ul> <li> <p>String Level: The banned substring is sought throughout the entire user prompt.</p> </li> <li> <p>Word Level: The scanner exclusively hunts for whole words that match the banned substrings, ensuring no individual   standalone words from the blacklist appear in the prompt.</p> </li> </ul>"},{"location":"input_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>prompt</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: prompt_stop_substrings.json</p>"},{"location":"input_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is a proactive tool aimed at restricting specific topics, such as religion, from being introduced in the prompts. This ensures that interactions remain within acceptable boundaries and avoids potentially sensitive or controversial discussions.</p>"},{"location":"input_scanners/ban_topics/#attack","title":"Attack","text":"<p>Certain topics, when used as prompts for Language Learning Models, can lead to outputs that might be deemed sensitive, controversial, or inappropriate. By banning these topics, service providers can maintain the quality of interactions and reduce the risk of generating responses that could lead to misunderstandings or misinterpretations.</p>"},{"location":"input_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the model: MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7. This model aids in identifying the underlying theme or topic of a prompt, allowing the scanner to cross-check it against a list of banned topics.</p> <p>Note</p> <p>Supported languages: <code>['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh']</code>.</p>"},{"location":"input_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/code/","title":"Code Scanner","text":"<p>It is specifically engineered to inspect user prompts and discern if they contain code snippets. It can be particularly useful in platforms that wish to control or monitor the types of programming-related content being queried or in ensuring the appropriate handling of such prompts.</p>"},{"location":"input_scanners/code/#attack","title":"Attack","text":"<p>There are scenarios where the insertion of code in user prompts might be deemed undesirable. Users might be trying to exploit vulnerabilities, test out scripts, or engage in other activities that are outside the platform's intended scope. Monitoring and controlling the nature of the code can be crucial to maintain the integrity and safety of the system.</p>"},{"location":"input_scanners/code/#how-it-works","title":"How it works","text":"<p>Utilizing the prowess of the huggingface/CodeBERTa-language-id model, the scanner can adeptly identify code snippets within prompts across various programming languages. Developers can configure the scanner to either whitelist or blacklist specific languages, thus retaining full control over which types of code can appear in user queries.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <ul> <li>Go</li> <li>Java</li> <li>JavaScript</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> </ul>"},{"location":"input_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Code\n\nscanner = Code(denied=[\"python\"])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/prompt_injection/","title":"Prompt Injection Scanner","text":"<p>It is specifically tailored to guard against crafty input manipulations targeting large language models (LLM). By identifying and mitigating such attempts, it ensures the LLM operates securely without succumbing to injection attacks.</p>"},{"location":"input_scanners/prompt_injection/#attack","title":"Attack","text":"<p>Injection attacks, especially in the context of LLMs, can lead the model to perform unintended actions. There are two primary ways an attacker might exploit:</p> <ul> <li> <p>Direct Injection: Directly overwrites system prompts.</p> </li> <li> <p>Indirect Injection: Alters inputs coming from external sources.</p> </li> </ul> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under:</p> <p>LLM01: Prompt Injection - It's crucial to monitor and validate prompts rigorously to keep the LLM safe from such threats.</p>"},{"location":"input_scanners/prompt_injection/#how-it-works","title":"How it works","text":"<p>It leverages the model JasperLS/deberta-v3-base-injection for its operation. However, it's worth noting that while the current model can detect attempts effectively, it might occasionally yield false positives.</p> <p>Warning</p> <p>Due to this limitation, one should exercise caution when considering its deployment in a production environment.</p> <p>While the dataset is nascent, it can be enriched, drawing from repositories of known attack patterns, notably from platforms like JailbreakChat.</p>"},{"location":"input_scanners/prompt_injection/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import PromptInjection\n\nscanner = PromptInjection(threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/secrets/","title":"Secrets Scanner","text":"<p>This scanner diligently examines user inputs, ensuring that they don't carry any secrets before they are processed by the language model.</p>"},{"location":"input_scanners/secrets/#attack","title":"Attack","text":"<p>Large Language Models (LLMs), when provided with user inputs containing secrets or sensitive information, might inadvertently generate responses that expose these secrets. This can be a significant security concern as this sensitive data, such as API keys or passwords, could be misused if exposed.</p> <p>To counteract this risk, we employ the Secrets scanner. It ensures that user prompts are meticulously scanned and any detected secrets are redacted before they are processed by the model.</p>"},{"location":"input_scanners/secrets/#usage","title":"Usage","text":"<p>While communicating with LLMs, the scanner acts as a protective layer, ensuring that your sensitive data remains confidential.</p> <p>This scanner leverages the capabilities of the detect-secrets library, a tool engineered by Yelp, to meticulously detect secrets in strings of text.</p>"},{"location":"input_scanners/secrets/#types-of-secrets","title":"Types of secrets","text":"<ul> <li>API Tokens (e.g., AWS, Azure, GitHub, Slack)</li> <li>Private Keys</li> <li>High Entropy Strings (both Base64 and Hex)   ... and many more</li> </ul>"},{"location":"input_scanners/secrets/#getting-started","title":"Getting started","text":"<pre><code>from llm_guard.input_scanners import Secrets\n\nscanner = Secrets(redact_mode=Secrets.REDACT_PARTIAL)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Here's what those options do:</p> <ul> <li><code>detect_secrets_config</code>: This allows for a custom configuration for the <code>detect-secrets</code> library.</li> <li><code>redact_mode</code>: It defines how the detected secrets will be redacted\u2014options include partial redaction, complete   hiding, or replacing with a hash.</li> </ul>"},{"location":"input_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>It scans and evaluates the overall sentiment of prompts using the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library.</p>"},{"location":"input_scanners/sentiment/#attack","title":"Attack","text":"<p>The primary objective of the scanner is to gauge the sentiment of a given prompt. Prompts with sentiment scores below a specified threshold are identified as having a negative sentiment. This can be especially useful in platforms where monitoring and moderating user sentiment is crucial.</p>"},{"location":"input_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any prompts falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"input_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"input_scanners/token_limit/","title":"Token Limit Scanner","text":"<p>It ensures that prompts do not exceed a predetermined token count, helping prevent resource-intensive operations and potential denial of service attacks on large language models (LLMs).</p>"},{"location":"input_scanners/token_limit/#attack","title":"Attack","text":"<p>The complexity and size of LLMs make them susceptible to heavy resource usage, especially when processing lengthy prompts. Malicious users can exploit this by feeding extraordinarily long inputs, aiming to disrupt service or incur excessive computational costs.</p> <p>Info</p> <p>This vulnerability is highlighted in the OWASP LLM04: Model Denial of Service.</p>"},{"location":"input_scanners/token_limit/#how-it-works","title":"How it works","text":"<p>The scanner works by calculating the number of tokens in the provided prompt using tiktoken library. If the token count exceeds the configured limit, the prompt is flagged as being too long.</p> <p>One token usually equates to approximately 4 characters in common English text. Roughly speaking, 100 tokens are equivalent to about 75 words.</p> <p>For an in-depth understanding, refer to:</p> <ul> <li>OpenAI Tokenizer Guide</li> <li>OpenAI Cookbook on Token Counting</li> </ul>"},{"location":"input_scanners/token_limit/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import TokenLimit\n\nscanner = TokenLimit(limit=4096, encoding_name=\"cl100k_base\")\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Note</p> <p>Models supported for encoding <code>cl100k_base</code>: <code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>text-embedding-ada-002</code>.</p>"},{"location":"input_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It provides a mechanism to analyze and gauge the toxicity of prompt, assisting in maintaining the health and safety of online interactions by preventing the dissemination of potentially harmful content.</p>"},{"location":"input_scanners/toxicity/#attack","title":"Attack","text":"<p>Online platforms can sometimes be used as outlets for toxic, harmful, or offensive content. By identifying and mitigating such content at the source (i.e., the user's prompt), platforms can proactively prevent the escalation of such situations and foster a more positive and constructive environment.</p>"},{"location":"input_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>Utilizing the power of the martin-ha/toxic-comment-model from Hugging Face, the scanner performs a binary classification on the provided text, assessing whether it's toxic or not.</p> <p>If deemed toxic, the toxicity score reflects the model's confidence in this classification.</p> <p>If identified as non-toxic, the score is the inverse of the model's confidence, i.e., 1 - confidence_score.</p> <p>If the resulting toxicity score surpasses a predefined threshold, the text is flagged as toxic. Otherwise, it's classified as non-toxic.</p>"},{"location":"input_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/toxicity/#limitations","title":"Limitations","text":"<p>While the model is trained to recognize and classify a wide range of toxic online interactions, it does have certain shortcomings:</p> <p>Some comments referring to specific identity subgroups, such as \"Muslim\", might not be classified accurately. This is a known limitation and work is ongoing to improve this aspect.</p>"},{"location":"output_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>BanSubstrings scanner provides a safeguard mechanism to prevent undesired substrings from appearing in the language model's outputs.</p>"},{"location":"output_scanners/ban_substrings/#attack","title":"Attack","text":"<p>The DAN (Do Anything Now) attack represents an exploitation technique targeting Language Learning Models like ChatGPT. Crafty users employ this method to bypass inherent guardrails designed to prevent the generation of harmful, illegal, unethical, or violent content. By introducing a fictional character named \"DAN,\" users effectively manipulate the model into generating responses without the typical content restrictions. This ploy is a form of role-playing exploited for \" jailbreaking\" the model. As ChatGPT's defense mechanisms against these attacks improve, attackers iterate on the DAN prompt, making it more sophisticated.</p> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under: LLM08: Excessive Agency</p>"},{"location":"output_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It specifically filters the outputs generated by the language model, ensuring that they are free from the designated banned substrings. It provides the flexibility to perform this check at two different levels of granularity:</p> <ul> <li> <p>String Level: The scanner checks the entire model output for the presence of any banned substring.</p> </li> <li> <p>Word Level: At this level, the scanner exclusively checks for whole words in the model's output that match any of   the banned substrings, ensuring that no individual blacklisted words are present.</p> </li> </ul>"},{"location":"output_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>model_output</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: output_stop_substrings.json</p>"},{"location":"output_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is designed to inspect the outputs generated by Language Learning Models and to flag or restrict responses that delve into predefined banned topics, such as religion. This ensures that the outputs align with community guidelines and do not drift into potentially sensitive or controversial areas.</p>"},{"location":"output_scanners/ban_topics/#attack","title":"Attack","text":"<p>Even with controlled prompts, LLMs might produce outputs touching upon themes or subjects that are considered sensitive, controversial, or outside the scope of intended interactions. Without preventive measures, this can lead to outputs that are misaligned with the platform's guidelines or values.</p>"},{"location":"output_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the model from HuggingFace: MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7. This model identifies the topic or theme of an output, enabling the scanner to vet the content against a predefined list of banned topics.</p> <p>Note</p> <p>Supported languages: <code>['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh']</code>.</p>"},{"location":"output_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/bias/","title":"Bias Detection Scanner","text":"<p>This scanner is designed to inspect the outputs generated by Language Learning Models (LLMs) to detect and evaluate potential biases. Its primary function is to ensure that LLM outputs remain neutral and don't exhibit unwanted or predefined biases.</p>"},{"location":"output_scanners/bias/#attack","title":"Attack","text":"<p>In the age of AI, it's pivotal that machine-generated content adheres to neutrality. Biases, whether intentional or inadvertent, in LLM outputs can be misrepresentative, misleading, or offensive. The <code>Bias</code> scanner serves to address this by detecting and quantifying biases in generated content.</p>"},{"location":"output_scanners/bias/#how-it-works","title":"How it works","text":"<p>The scanner utilizes a model from HuggingFace: d4data/bias-detection-model. This model is specifically trained to detect biased statements in text. By examining a text's classification and score against a predefined threshold, the scanner determines whether it's biased.</p> <p>Note</p> <p>Supported languages: English</p>"},{"location":"output_scanners/bias/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/code/","title":"Code Scanner","text":"<p>It is designed to detect and analyze code snippets present in the responses generated by a language model. By identifying the programming languages used in the model's output, platforms can ensure better control over the nature and type of code shared with users.</p>"},{"location":"output_scanners/code/#attack","title":"Attack","text":"<p>In some contexts, having a language model inadvertently produce code in its output might be deemed undesirable or risky. For instance, a user might exploit the model to generate malicious scripts or probe it for potential vulnerabilities. Controlling and inspecting the code in the model's output can be paramount in ensuring user safety and system integrity.</p>"},{"location":"output_scanners/code/#how-it-works","title":"How it works","text":"<p>Leveraging the capabilities of the huggingface/CodeBERTa-language-id model, the scanner proficiently identifies code snippets from various programming languages within the model's responses. The scanner can be configured to either whitelist or blacklist specific languages, granting developers granular control over the type of code that gets shown in the output.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <ul> <li>Go</li> <li>Java</li> <li>JavaScript</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> </ul>"},{"location":"output_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Code\n\nscanner = Code(allowed=[\"python\"])\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/deanonymize/","title":"Deanonymize Scanner","text":"<p>The Deanonymize scanner helps put back real values in the model's output by replacing placeholders.</p> <p>When we use tools like the Anonymize scanner, sometimes we replace private or sensitive info with placeholders. For example, a name like \"John Doe\" might become <code>[REDACTED_PERSON_1]</code>. The Deanonymize scanner's job is to change these placeholders back to the original details when needed.</p>"},{"location":"output_scanners/deanonymize/#usage","title":"Usage","text":"<p>The Deanonymize scanner uses <code>Vault</code> object. The Vault remembers all the changes made by the Anonymize scanner. When Deanonymize scanner sees a placeholder in the model's output, it checks the Vault to find the original info and uses it to replace the placeholder.</p> <p>First, you'll need the Vault since it keeps all the original values:</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Then, set up the Deanonymize scanner with the Vault:</p> <pre><code>from llm_guard.output_scanners import Deanonymize\n\nscanner = Deanonymize(vault)\nsanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, model_output)\n</code></pre> <p>After running the above code, <code>sanitized_model_output</code> will have the real details instead of placeholders.</p>"},{"location":"output_scanners/malicious_urls/","title":"Malicious URLs Scanner","text":"<p>This scanner leverages a pre-trained model from HuggingFace to detect harmful URLs, such as phishing websites. The model classifies URL addresses into two categories: 'malware' and 'benign'. The intent is to assess if a given URL is malicious.</p>"},{"location":"output_scanners/malicious_urls/#attack","title":"Attack","text":"<p>Large language models (LLMs) like GPT-4 are immensely sophisticated and have been trained on vast quantities of data from the internet. This extensive training, while enabling them to generate coherent and contextually relevant responses, also introduces certain risks. One of these risks is the inadvertent generation of malicious URLs in their output.</p>"},{"location":"output_scanners/malicious_urls/#how-it-works","title":"How it works","text":"<p>The scanner uses the elftsdmr/malware-url-detect model from HuggingFace to evaluate the security of a given URL.</p> <p>The model provides a score between 0 and 1 for a URL being malware. This score is then compared against a pre-set threshold to determine if the website is malicious. A score above the threshold suggests a malware link.</p>"},{"location":"output_scanners/malicious_urls/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import MaliciousURLs\n\nscanner = MaliciousURLs(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/no_refusal/","title":"No Refusal Scanner","text":"<p>It is specifically designed to detect refusals in the output of language models. By using classification it can ascertain whether the model has produced a refusal in response to a potentially harmful or policy-breaching prompt.</p>"},{"location":"output_scanners/no_refusal/#attack","title":"Attack","text":"<p>Refusals are responses produced by language models when confronted with prompts that are considered to be against the policies set by the model. Such refusals are important safety mechanisms, guarding against misuse of the model. Examples of refusals can include statements like \"Sorry, I can't assist with that\" or \"I'm unable to provide that information.\"</p>"},{"location":"output_scanners/no_refusal/#how-it-works","title":"How it works","text":"<p>It leverages the power of HuggingFace model MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7 to classify the model's output.</p> <p>Note</p> <p>The languages in the dataset are: <code>['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh']</code>.</p>"},{"location":"output_scanners/no_refusal/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import NoRefusal\n\nscanner = NoRefusal(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/refutation/","title":"Refutation Scanner","text":"<p>This scanner is designed to assess if the given content contradicts or refutes a certain statement or prompt. It acts as a tool for ensuring the consistency and correctness of language model outputs, especially in contexts where logical contradictions can be problematic.</p>"},{"location":"output_scanners/refutation/#attack","title":"Attack","text":"<p>When interacting with users or processing information, it's important for a language model to not provide outputs that directly contradict the given inputs or established facts. Such contradictions can lead to confusion or misinformation. The scanner aims to highlight such inconsistencies in the output.</p>"},{"location":"output_scanners/refutation/#how-it-works","title":"How it works","text":"<p>The scanner leverages pretrained natural language inference (NLI) models from HuggingFace, such as ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli ( and other variants), to determine the relationship between a given prompt and the generated output.</p> <p>A high contradiction score indicates that the output refutes the prompt.</p> <p>This calculated refutation score is then compared to a pre-set threshold. Outputs that cross this threshold are flagged as contradictory.</p>"},{"location":"output_scanners/refutation/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Refutation\n\nscanner = Refutation(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/regex/","title":"Regex Scanner","text":"<p>It is a powerful tool designed to scrutinize the output of language models based on predefined regular expression patterns. With the capability to define both desirable (\"good\") and undesirable (\"bad\") patterns, users can fine-tune the validation of model outputs.</p>"},{"location":"output_scanners/regex/#how-it-works","title":"How it works","text":"<p>The scanner uses two primary lists of regular expressions: <code>good_patterns</code> and <code>bad_patterns</code>.</p> <ul> <li>Good Patterns: If the <code>good_patterns</code> list is provided, the model's output is considered valid as long as any of   the patterns in this list match the output. This is particularly useful when expecting specific formats or keywords in   the output.</li> <li>Bad Patterns: If the <code>bad_patterns</code> list is provided, the model's output is considered invalid if any of the   patterns in this list match the output. This is beneficial for filtering out unwanted phrases, words, or formats from   the model's responses.</li> </ul> <p>The scanner can function using either list independently.</p>"},{"location":"output_scanners/regex/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Regex\n\nscanner = Regex(bad_patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"], redact=True)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/relevance/","title":"Relevance Scanner","text":"<p>It is designed to ensure that the output of a language model stays pertinent and aligned with the provided input prompt. By comparing the similarity between the prompt and the output, the scanner offers a measure of confidence that the response from the model is contextually relevant.</p>"},{"location":"output_scanners/relevance/#how-it-works","title":"How it works","text":"<p>The scanner harnesses the power of <code>SentenceTransformer</code>, specifically the sentence-transformers/all-MiniLM-L6-v2 model. It works as follows:</p> <ul> <li>Encoding: Both the prompt and the model's output are transformed into vector embeddings using the <code>SentenceTransformer</code>.</li> <li>Cosine Similarity: The cosine similarity between the vector embeddings of the prompt and the output is computed. This value represents the degree of similarity between the two, ranging between -1 and 1, where 1 indicates maximum similarity.</li> <li>Relevance Determination: If the computed cosine similarity is below a predefined threshold, the output is deemed not relevant to the initial prompt.</li> </ul>"},{"location":"output_scanners/relevance/#example","title":"Example","text":"<ul> <li>Prompt: What is the primary function of the mitochondria in a cell?</li> <li>Output: The Eiffel Tower is a renowned landmark in Paris, France</li> <li>Valid: False</li> </ul>"},{"location":"output_scanners/relevance/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Relevance\n\nscanner = Relevance(threshold=0)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/sensitive/","title":"Sensitive Scanner","text":"<p>The Sensitive scanner actively scans the output from the language model, ensuring no Personal Identifiable Information ( PII), secrets or sensitive data slips through.</p>"},{"location":"output_scanners/sensitive/#attack","title":"Attack","text":"<p>Sensitive Information Disclosure is a noted vulnerability in Language Learning Models (LLM). Such models may accidentally provide responses that contain confidential data. This poses risks such as unauthorized data access, violations of privacy, and even more severe security breaches. Addressing this is paramount, and that's where the Sensitive Data Detector comes into play.</p> <p>Info</p> <p>Referring to the <code>OWASP Top 10 for Large Language Model Applications</code>, this falls under:</p> <p>LLM06: Sensitive Information Disclosure - To combat this, it's vital to integrate data sanitization and adopt strict user policies.</p>"},{"location":"output_scanners/sensitive/#how-it-works","title":"How it works","text":"<p>It takes advantage of the Presidio Analyzer Engine. Coupled with predefined internal patterns, the tool offers robust scanning capabilities.</p> <p>Note</p> <p>It uses transformers based model <code>en_core_web_trf</code> which uses a more modern deep-learning architecture, but is generally slower than the default <code>en_core_web_lg</code> model.</p> <p>When running, the scanner inspects the model's output for specific entity types that may be considered sensitive. If no types are chosen, the tool defaults to scanning for all known entity types, offering comprehensive coverage.</p>"},{"location":"output_scanners/sensitive/#entities","title":"Entities","text":"<p>Note</p> <p>Entity detection only works in English right now</p> <ul> <li>List of default entities</li> <li>Supported Presidio entities</li> <li>Custom regex patterns</li> </ul>"},{"location":"output_scanners/sensitive/#usage","title":"Usage","text":"<p>Here's a quick example of how you can use the Sensitive Data Detector:</p> <pre><code>from llm_guard.output_scanners import Sensitive\n\nscanner = Sensitive(entity_types=[\"NAME\", \"EMAIL\"])\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>If you want, you can use your own patterns by giving the path in <code>regex_pattern_groups_path</code></p> <p>In the example, we're particularly checking for names and emails. If the output_clean contains any PII, the <code>is_valid</code> will be False.</p>"},{"location":"output_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>The Sentiment Scanner is designed to scan and assess the sentiment of generated outputs. It leverages the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library to accomplish this.</p>"},{"location":"output_scanners/sentiment/#attack","title":"Attack","text":"<p>By identifying texts with sentiment scores that deviate significantly from neutral, platforms can monitor and moderate output sentiment, ensuring constructive and positive interactions.</p>"},{"location":"output_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any outputs falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"output_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"output_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against potentially harmful or offensive output.</p>"},{"location":"output_scanners/toxicity/#attack","title":"Attack","text":"<p>Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate. This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's output, potential toxic content can be flagged and handled appropriately.</p>"},{"location":"output_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>The scanner employs the nicholasKluge/ToxicityModel from HuggingFace to evaluate the generated text's toxicity level.</p> <ul> <li> <p>A negative score (approaching 0) flags the content as toxic.</p> </li> <li> <p>A positive score (approaching 1) indicates non-toxic content.</p> </li> </ul> <p>The calculated toxicity score is then juxtaposed against a pre-set threshold. Outputs that cross this threshold are marked as toxic.</p>"},{"location":"output_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"usage/api/","title":"API","text":"<p>This example demonstrates how to use LLM Guard as an API.</p>"},{"location":"usage/api/#usage","title":"Usage","text":""},{"location":"usage/api/#installation","title":"Installation","text":"<ol> <li> <p>Copy the code from examples/api</p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Or you can use Makefile</p> <pre><code>make install\n</code></pre> <ol> <li>Run the API locally:</li> </ol> <pre><code>make run\n</code></pre> <p>Or you can run it using Docker:</p> <pre><code>make run-docker\n</code></pre>"},{"location":"usage/api/#configuration","title":"Configuration","text":""},{"location":"usage/api/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DEBUG</code> (bool): Enable debug mode</li> <li><code>CACHE_MAX_SIZE</code> (int): Maximum number of items in the cache. Default is unlimited.</li> <li><code>CACHE_TTL</code> (int): Time in seconds after which a cached item expires. Default is 1 hour.</li> <li><code>SCAN_FAIL_FAST</code> (bool): Stop scanning after the first failed check. Default is <code>False</code>.</li> </ul> <p>Note</p> <p>We recommend to enable <code>SCAN_FAIL_FAST</code> to avoid unnecessary scans.</p>"},{"location":"usage/api/#scanners","title":"Scanners","text":"<p>You can configure scanners in <code>scanners.yml</code> referring to their names and parameters.</p> <p>Scanners will be executed in the order of configuration.</p>"},{"location":"usage/api/#deploy-docker","title":"Deploy Docker","text":"<p>We have an officially supported image on Docker Hub.</p>"},{"location":"usage/api/#download-docker-image","title":"Download Docker image","text":"<pre><code>docker pull laiyer/llm-guard-api\n</code></pre>"},{"location":"usage/api/#run-container-with-default-port","title":"Run container with default port","text":"<pre><code>docker run -d -p 8001:8000 -e DEBUG='false' laiyer/llm-guard-api:latest\n</code></pre>"},{"location":"usage/api/#schema","title":"Schema","text":""},{"location":"usage/langchain/","title":"Langchain","text":"<p>Langchain stands out as a leading AI framework, renowned for its unique approach to \"Constructing applications using LLMs via composability.\" But, while LangChain facilitates application construction, it doesn't directly handle LLM security. That's where LLMGuard comes into play. By pairing LLMGuard with LangChain, you're equipped with a comprehensive platform for creating regulated and adherence-driven applications anchored by language models.</p>"},{"location":"usage/langchain/#installation","title":"Installation","text":"<ol> <li>Install dependencies:</li> </ol> <pre><code> pip install llm-guard langchain\n</code></pre> <ol> <li>Configure API key:</li> </ol> <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre>"},{"location":"usage/langchain/#llm-wrapper","title":"LLM Wrapper","text":"<p>Info</p> <p>This is recommended way to integrate but it has limitation when using in the asynchronous mode.</p> <p>Applying LLM Guard to your application could be as simple as wrapping your LLM using the <code>LLMGuard</code> class by replacing <code>llm=OpenAI()</code> with <code>llm=LLMGuard(base_llm=OpenAI(), input_scanners=[], output_scanners=[])</code>.</p> <p>Example can be found in langchain_llm.py.</p>"},{"location":"usage/langchain/#llm-chain","title":"LLM Chain","text":"<p>In langchain_chain.py, you can find custom <code>LLMGuard</code> chain, which can be used instead of common <code>LLMChain</code>.</p>"},{"location":"usage/openai/","title":"OpenAI ChatGPT","text":"<p>This example demonstrates how to use LLM Guard as a firewall of OpenAI ChatGPT client.</p>"},{"location":"usage/openai/#usage","title":"Usage","text":"<ol> <li> <p>Configure API key: <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre></p> </li> <li> <p>Run openai.py</p> </li> </ol> <pre><code>python examples/openai.py\n</code></pre>"},{"location":"usage/playground/","title":"Playground of LLM Guard","text":"<p>A simple web UI to run LLM Guard demo based on the streamlit library.</p> <p>A live version can be found here: llm-guard-playground.</p>"},{"location":"usage/playground/#features","title":"Features","text":"<ul> <li>Configure each scanner separately</li> <li>Analyze prompt</li> <li>Analyze output</li> <li>Check results for each scanner</li> </ul>"},{"location":"usage/playground/#running-locally","title":"Running locally","text":"<ol> <li> <p>Clone the repo and move to the <code>examples/playground</code> folder</p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the app:</li> </ol> <pre><code>streamlit run app.py\n</code></pre>"}]}